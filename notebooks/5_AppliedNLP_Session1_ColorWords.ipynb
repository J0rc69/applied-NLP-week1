{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad6d3fe8",
   "metadata": {},
   "source": [
    "# 5) Color Words & Description Density\n",
    "\n",
    "**Goal:** Count color terms and compare description density."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b54338f",
   "metadata": {},
   "source": [
    "# Setup: Load Texts\n",
    "\n",
    "This notebook needs **Alice in Wonderland** and **Through the Looking-Glass** as input texts.\n",
    "\n",
    "**How to provide the texts:**\n",
    "1. Download books from Project Gutenberg (IDs 11 and 12) as txts. [go to https://www.gutenberg.org/ebooks/11 and https://www.gutenberg.org/ebooks/12]\n",
    "\n",
    "2. Place two text files in the \"data\" folder with names:\n",
    "   - `Wondeland.txt`  (Alice's Adventures in Wonderland)\n",
    "   - `Looking-Glass.txt` (Through the Looking-Glass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5181c2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b7911c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_texts(local_alice: str = '../data/Wonderland.txt',\n",
    "               local_glass: str = '../data/Looking-Glass.txt'):\n",
    "    \"\"\"Load Wonderland and Looking-Glass texts from disk.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    local_alice : str\n",
    "        Path to Wonderland text file. Defaults to '../data/Wonderland.txt'.\n",
    "    local_glass : str\n",
    "        Path to Looking-Glass text file. Defaults to '../data/Looking-Glass.txt'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[str, str]\n",
    "        (wonderland_text, lookingglass_text).\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    FileNotFoundError\n",
    "        If either file is missing.\n",
    "\n",
    "    Extra Notes\n",
    "    -----------\n",
    "    - Using UTF-8 with `errors='ignore'` avoids codec exceptions on\n",
    "      older Project Gutenberg dumps or inconsistent encodings.\n",
    "    \"\"\"\n",
    "    p1, p2 = Path(local_alice), Path(local_glass)\n",
    "\n",
    "    # Fail fast with a clear message if a file is missing\n",
    "    if not p1.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Missing file: {p1}\\n\"\n",
    "            \"→ Please place 'Wonderland.txt' at this path or update load_texts(...).\"\n",
    "        )\n",
    "    if not p2.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Missing file: {p2}\\n\"\n",
    "            \"→ Please place 'Looking-Glass.txt' at this path or update load_texts(...).\"\n",
    "        )\n",
    "\n",
    "    # Read the files (UTF-8; ignore undecodable bytes to stay robust)\n",
    "    wonderland   = p1.read_text(encoding='utf-8', errors='ignore')\n",
    "    lookingglass = p2.read_text(encoding='utf-8', errors='ignore')\n",
    "    return wonderland, lookingglass\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "    \"\"\"Normalize a Gutenberg-like text for tokenization.\n",
    "\n",
    "    Steps\n",
    "    -----\n",
    "    1) Heuristically strip Project Gutenberg headers/footers if present\n",
    "       (looks for *** START ... *** END markers).\n",
    "    2) Normalize newlines to '\\n'.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        Raw text as loaded from disk (can be empty).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Cleaned text suitable for tokenization and counting.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return ''\n",
    "    # Clip to the main body if markers are present.\n",
    "    start = text.find('*** START')\n",
    "    end   = text.find('*** END')\n",
    "    if start != -1 and end != -1 and end > start:\n",
    "        text = text[start:end]\n",
    "    # Normalize Windows line endings.\n",
    "    return text.replace('\\r\\n', '\\n')\n",
    "\n",
    "# Load raw texts (forgiving: returns '' if a file is missing)\n",
    "wonderland_raw, lookingglass_raw = load_texts()\n",
    "\n",
    "# Normalize for tokenization\n",
    "wonderland   = normalize(wonderland_raw)\n",
    "lookingglass = normalize(lookingglass_raw)\n",
    "\n",
    "print(f\"Wonderland chars: {len(wonderland):,} | Looking-Glass chars: {len(lookingglass):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede60614",
   "metadata": {},
   "source": [
    "### Helpers: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87087b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_RE = re.compile(r\"[A-Za-z']+\")  # keep apostrophes in words (e.g., don't -> don't)\n",
    "\n",
    "def words(text: str):\n",
    "    \"\"\"Simple word tokenizer (lowercased, ASCII letters + apostrophes).\n",
    "\n",
    "    Pros\n",
    "    ----\n",
    "    - Very fast and dependency-free.\n",
    "    - Good enough for frequency/keyness demonstrations.\n",
    "\n",
    "    Cons\n",
    "    ----\n",
    "    - No punctuation words, no sentence boundaries, no POS tags.\n",
    "    - May treat possessives inconsistently across sources.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[str]\n",
    "        Lowercased word words.\n",
    "    \"\"\"\n",
    "    return WORD_RE.findall(text.lower())\n",
    "\n",
    "\n",
    "def sentences(text: str):\n",
    "    \"\"\"Naive sentence splitter using punctuation boundaries.\n",
    "\n",
    "    Uses a regex to split on '.', '!', '?' followed by whitespace.\n",
    "    Because this is heuristic, treat results as approximate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[str]\n",
    "        Sentence-like strings.\n",
    "    \"\"\"\n",
    "    return [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n",
    "\n",
    "\n",
    "\n",
    "wonderland_words = words(wonderland)\n",
    "lookingglass_words = words(lookingglass)\n",
    "\n",
    "wonderland_sentences = sentences(wonderland)\n",
    "lookingglass_sentences = sentences(lookingglass)\n",
    "\n",
    "print(f\"Wonderland words: {len(wonderland_words):,} | Looking-Glass words: {len(lookingglass_words):,}\")\n",
    "print(f\"Wonderland sentences: {len(wonderland_sentences):,} | Looking Glass sentences: {len(lookingglass_sentences):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4d223d",
   "metadata": {},
   "source": [
    "### Count Color Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2957c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLOR_LIST = {\n",
    "    'red','orange','yellow','green','blue','indigo','violet','purple','pink','brown','black','white','gray','grey',\n",
    "    'scarlet','crimson','emerald','amber','gold','silver','lavender','mauve','ivory','beige','teal','turquoise','magenta','maroon','navy'\n",
    "}\n",
    "def count_colors(tokens):\n",
    "    c = Counter(w for w in tokens if w in COLOR_LIST)\n",
    "    return c, sum(c.values()), len(tokens)\n",
    "\n",
    "a_c, a_hits, a_total = count_colors(wonderland_words)\n",
    "g_c, g_hits, g_total = count_colors(lookingglass_words)\n",
    "print(\"Alice top:\", a_c.most_common(15), \"| rate per 100k:\", (a_hits/a_total)*100000)\n",
    "print(\"Glass top:\", g_c.most_common(15), \"| rate per 100k:\", (g_hits/g_total)*100000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4903cf14",
   "metadata": {},
   "source": [
    "**Discuss:** Where do color bursts cluster in the narrative? What scenes rely on color to signal mood or magic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418056cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_color_windows(tokens, window=800, step=200, color_list=COLOR_LIST):\n",
    "    hits_per_window = []\n",
    "    for i in range(0, max(1, len(tokens)-window+1), step):\n",
    "        chunk = tokens[i:i+window]\n",
    "        c = sum(1 for w in chunk if w in color_list)\n",
    "        hits_per_window.append((i, i+window, c, c * (100000/window)))  # per 100k\n",
    "    return hits_per_window\n",
    "\n",
    "def nearest_sentence_span(tokens, sents, start_idx, end_idx):\n",
    "    # approximate: map token range to a sentence slice of similar length\n",
    "    # (good enough for previewing passages)\n",
    "    text = \" \".join(tokens[start_idx:end_idx])\n",
    "    # find a sentence that contains first few words of the window\n",
    "    head = \" \".join(tokens[start_idx:start_idx+20])\n",
    "    for k, s in enumerate(sents):\n",
    "        if head[:50] in s:\n",
    "            j0 = max(0, k-1); j1 = min(len(sents), k+3)\n",
    "            return j0, j1, \" \".join(sents[j0:j1])\n",
    "    return None, None, text[:500]\n",
    "\n",
    "# run it\n",
    "w_roll = rolling_color_windows(wonderland_words, window=800, step=200)\n",
    "g_roll = rolling_color_windows(lookingglass_words, window=800, step=200)\n",
    "\n",
    "# top 5 bursts by per-100k\n",
    "w_top = sorted(w_roll, key=lambda x: x[3], reverse=True)[:5]\n",
    "g_top = sorted(g_roll, key=lambda x: x[3], reverse=True)[:5]\n",
    "\n",
    "print(\"=== Wonderland color bursts ===\")\n",
    "for a,b,h,rate in w_top:\n",
    "    j0,j1,preview = nearest_sentence_span(wonderland_words, wonderland_sentences, a,b)\n",
    "    print(f\"[tokens {a}-{b}] hits={h} | {rate:.0f} per 100k\")\n",
    "    print(preview[:400], \"…\\n\")\n",
    "\n",
    "print(\"=== Looking-Glass color bursts ===\")\n",
    "for a,b,h,rate in g_top:\n",
    "    j0,j1,preview = nearest_sentence_span(lookingglass_words, lookingglass_sentences, a,b)\n",
    "    print(f\"[tokens {a}-{b}] hits={h} | {rate:.0f} per 100k\")\n",
    "    print(preview[:400], \"…\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c2d4cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
