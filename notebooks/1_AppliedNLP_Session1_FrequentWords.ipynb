{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a05e673",
   "metadata": {},
   "source": [
    "\n",
    "# 1) Frequent Words = Literary Fingerprints\n",
    "\n",
    "This notebook compares **word frequency** between our two toy texts:\n",
    "- *Alice's Adventures in Wonderland* (here referenced as **Wonderland**)\n",
    "- *Through the Looking-Glass* (here referenced as **Looking-Glass**)\n",
    "\n",
    "We practice simple tokenization and frequency analysis, then discuss\n",
    "what's **meaningful signal** vs. **noise** in the results, and how to\n",
    "improve the method (normalization, keyness, etc).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d25e38",
   "metadata": {},
   "source": [
    "# Setup: Load Texts\n",
    "\n",
    "This notebook needs **Alice in Wonderland** and **Through the Looking-Glass** as input texts.\n",
    "\n",
    "**How to provide the texts:**\n",
    "1. Download books from Project Gutenberg (IDs 11 and 12) as txts. [go to https://www.gutenberg.org/ebooks/11 and https://www.gutenberg.org/ebooks/12]\n",
    "\n",
    "2. Place two text files in the \"data\" folder with names:\n",
    "   - `Wondeland.txt`  (Alice's Adventures in Wonderland)\n",
    "   - `Looking-Glass.txt` (Through the Looking-Glass)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2f5446",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5da6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_texts(local_alice: str = '../data/Wonderland.txt',\n",
    "               local_glass: str = '../data/Looking-Glass.txt'):\n",
    "    \"\"\"Load Wonderland and Looking-Glass texts from disk.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    local_alice : str\n",
    "        Path to Wonderland text file. Defaults to '../data/Wonderland.txt'.\n",
    "    local_glass : str\n",
    "        Path to Looking-Glass text file. Defaults to '../data/Looking-Glass.txt'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[str, str]\n",
    "        (wonderland_text, lookingglass_text).\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    FileNotFoundError\n",
    "        If either file is missing.\n",
    "\n",
    "    Extra Notes\n",
    "    -----------\n",
    "    - Using UTF-8 with `errors='ignore'` avoids codec exceptions on\n",
    "      older Project Gutenberg dumps or inconsistent encodings.\n",
    "    \"\"\"\n",
    "    p1, p2 = Path(local_alice), Path(local_glass)\n",
    "\n",
    "    # Fail fast with a clear message if a file is missing\n",
    "    if not p1.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Missing file: {p1}\\n\"\n",
    "            \"→ Please place 'Wonderland.txt' at this path or update load_texts(...).\"\n",
    "        )\n",
    "    if not p2.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Missing file: {p2}\\n\"\n",
    "            \"→ Please place 'Looking-Glass.txt' at this path or update load_texts(...).\"\n",
    "        )\n",
    "\n",
    "    # Read the files (UTF-8; ignore undecodable bytes to stay robust)\n",
    "    wonderland   = p1.read_text(encoding='utf-8', errors='ignore')\n",
    "    lookingglass = p2.read_text(encoding='utf-8', errors='ignore')\n",
    "    return wonderland, lookingglass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64e2892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text: str) -> str:\n",
    "    \"\"\"Normalize a Gutenberg-like text for tokenization.\n",
    "\n",
    "    Steps\n",
    "    -----\n",
    "    1) Heuristically strip Project Gutenberg headers/footers if present\n",
    "       (looks for *** START ... *** END markers).\n",
    "    2) Normalize newlines to '\\n'.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        Raw text as loaded from disk (can be empty).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Cleaned text suitable for tokenization and counting.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return ''\n",
    "    # Clip to the main body if markers are present.\n",
    "    start = text.find('*** START')\n",
    "    end   = text.find('*** END')\n",
    "    if start != -1 and end != -1 and end > start:\n",
    "        text = text[start:end]\n",
    "    # Normalize Windows line endings.\n",
    "    return text.replace('\\r\\n', '\\n')\n",
    "\n",
    "\n",
    "# -------- Tokenization helpers (simple) --------\n",
    "\n",
    "WORD_RE = re.compile(r\"[A-Za-z']+\")  # keep apostrophes in words (e.g., don't -> don't)\n",
    "\n",
    "def words(text: str):\n",
    "    \"\"\"Simple word tokenizer (lowercased, ASCII letters + apostrophes).\n",
    "\n",
    "    Pros\n",
    "    ----\n",
    "    - Very fast and dependency-free.\n",
    "    - Good enough for frequency/keyness demonstrations.\n",
    "\n",
    "    Cons\n",
    "    ----\n",
    "    - No punctuation words, no sentence boundaries, no POS tags.\n",
    "    - May treat possessives inconsistently across sources.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[str]\n",
    "        Lowercased word words.\n",
    "    \"\"\"\n",
    "    return WORD_RE.findall(text.lower())\n",
    "\n",
    "\n",
    "def sentences(text: str):\n",
    "    \"\"\"Naive sentence splitter using punctuation boundaries.\n",
    "\n",
    "    Uses a regex to split on '.', '!', '?' followed by whitespace.\n",
    "    Because this is heuristic, treat results as approximate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[str]\n",
    "        Sentence-like strings.\n",
    "    \"\"\"\n",
    "    return [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n",
    "\n",
    "\n",
    "# -------- Frequency & keyness utilities --------\n",
    "\n",
    "def top_words(words, min_len=4, extra_stop=None, n=30):\n",
    "    \"\"\"Return top-N frequent words after lightweight filtering.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    words : list[str]\n",
    "        Word words (e.g., output of `words()`).\n",
    "    min_len : int\n",
    "        Minimum length to keep (filters very short function words).\n",
    "    extra_stop : Iterable[str] or None\n",
    "        Additional stopwords to exclude (e.g., {'just','only'}).\n",
    "    n : int\n",
    "        Number of items to return.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[tuple[str,int]]\n",
    "        Top-N (word, count) pairs.\n",
    "    \"\"\"\n",
    "    base_stop = {\n",
    "        'the','and','to','of','a','i','it','in','that','was','he','you','is','for','on','as',\n",
    "        'with','his','her','at','be','she','had','not','but','said','they','them','this','so','all','one','very',\n",
    "        'there','what','were','from','have','would','could','when','been','their','we','my','me','or','by','up','no','out','if',\n",
    "        # book-specific names are often noise for stylistics; tweak as needed\n",
    "        'alice'\n",
    "    }\n",
    "    if extra_stop:\n",
    "        base_stop |= set(extra_stop)\n",
    "\n",
    "    c = Counter(w for w in words if len(w) >= min_len and w not in base_stop)\n",
    "    return c.most_common(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618c7d96",
   "metadata": {},
   "source": [
    "\n",
    "## Load & Normalize\n",
    "We load both texts using **inline path checks** and then apply a simple normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbfb253",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load raw texts (forgiving: returns '' if a file is missing)\n",
    "wonderland_raw, lookingglass_raw = load_texts()\n",
    "\n",
    "# Normalize for tokenization\n",
    "wonderland   = normalize(wonderland_raw)\n",
    "lookingglass = normalize(lookingglass_raw)\n",
    "\n",
    "print(f\"Wonderland chars: {len(wonderland):,} | Looking-Glass chars: {len(lookingglass):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b194521",
   "metadata": {},
   "source": [
    "\n",
    "## Tokenize\n",
    "We use a simple regex tokenizer (letters + apostrophes). For more serious work,\n",
    "consider spaCy or stanza for tagging and lemmatization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a843aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wonderland_words = words(wonderland)\n",
    "lookingglass_words = words(lookingglass)\n",
    "\n",
    "wonderland_sentences = sentences(wonderland)\n",
    "lookingglass_sentences = sentences(lookingglass)\n",
    "\n",
    "print(f\"Wonderland words: {len(wonderland_words):,} | Looking-Glass words: {len(lookingglass_words):,}\")\n",
    "print(f\"Wonderland sentences: {len(wonderland_sentences):,} | Looking Glass sentences: {len(lookingglass_sentences):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188dd619",
   "metadata": {},
   "source": [
    "\n",
    "## Top Words (after basic stopwords)\n",
    "The list is **partly signal, partly noise**—use it to start discussion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec91f70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Top Wonderland:\", top_words(wonderland_words)[:15])\n",
    "print(\"Top Looking-Glass:\", top_words(lookingglass_words)[:15])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10829309",
   "metadata": {},
   "source": [
    "\n",
    "## Discussion\n",
    "- Which frequent words are **thematically meaningful** vs. artifacts of stopwording?\n",
    "- Do **chess terms** (e.g., *queen*, *white*, *red*) show higher distinctiveness in *Looking-Glass*?\n",
    "- Do **spatial/falling terms** (e.g., *down*, *rabbit*) show higher distinctiveness in *Wonderland*?\n",
    "- How would **lemmatization** (e.g., *think/thinks/thought*) change results?\n",
    "- Implement **per_10k(count,total_words)** and **lolookingglass_likelihood(k1,n1,k2,n2) (Dunning’s G²)**, then list the 20 most distinctive words between Wonderland and Looking-Glass with per-10k rates and briefly argue which are meaningful vs. artifacts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36d4fbd",
   "metadata": {},
   "source": [
    "## Optional continution:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2825a8f8",
   "metadata": {},
   "source": [
    "\n",
    "## Distinctiveness via Log-Likelihood (Keyness)\n",
    "Raw frequency is not enough. Compute **G²** to find words that are *distinctive* of each book.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285caabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_10k(count: int, total_words: int) -> float:\n",
    "    \"\"\"Normalize a raw count per 10,000 words for fair comparisons.\"\"\"\n",
    "    return (count / max(1, total_words)) * 10000.0\n",
    "\n",
    "\n",
    "def lolookingglass_likelihood(k1: int, n1: int, k2: int, n2: int) -> float:\n",
    "    \"\"\"Dunning’s log-likelihood (G^2) keyness score for word distinctiveness.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    k1 : int  Frequency in corpus A\n",
    "    n1 : int  Total words in corpus A\n",
    "    k2 : int  Frequency in corpus B\n",
    "    n2 : int  Total words in corpus B\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        G^2 value; larger absolute values indicate stronger distinctiveness.\n",
    "        Direction should be interpreted by comparing rates (per_10k) or counts.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Symmetric measure widely used for corpus comparison.\n",
    "    - Great classroom upgrade over raw frequency lists.\n",
    "    \"\"\"\n",
    "    E1 = n1 * (k1 + k2) / max(1, (n1 + n2))\n",
    "    E2 = n2 * (k1 + k2) / max(1, (n1 + n2))\n",
    "\n",
    "    def term(k, E):\n",
    "        return 0.0 if k == 0 or E == 0 else k * math.log(k / E)\n",
    "\n",
    "    return 2.0 * (term(k1, E1) + term(k2, E2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d91d5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build frequency dictionaries\n",
    "cw = Counter(wonderland_words)\n",
    "cg = Counter(lookingglass_words)\n",
    "nW, nG = sum(cw.values()), sum(cg.values())\n",
    "\n",
    "# Compare a candidate set (union of top ~500 from each to keep it fast)\n",
    "candidates = set([w for w,_ in cw.most_common(500)] + [w for w,_ in cg.most_common(500)])\n",
    "\n",
    "rows = []\n",
    "for w in candidates:\n",
    "    g2 = lolookingglass_likelihood(cw[w], nW, cg[w], nG)\n",
    "    rows.append((g2, w, per_10k(cw[w], nW), per_10k(cg[w], nG)))\n",
    "\n",
    "# Sort by distinctiveness (descending)\n",
    "rows.sort(reverse=True)\n",
    "\n",
    "print(\"Most distinctive (either direction):\")\n",
    "for g2, w, a10, b10 in rows[:20]:\n",
    "    print(f\"{w:>12}  G2={g2:7.1f}  W:{a10:6.2f}/10k  LG:{b10:6.2f}/10k\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a16670a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
